<!DOCTYPE html>
<html lang="en">

<!-- 
    _____  _                              _        _  ___  _               _     _ 
   / ____|| |                            | |      | |/ / |(_)             | |   (_)
  | (___  | |__   _   _  _ __   ___  _  _| | ___  | ' /| | _  _ __   _   _| |__  _ 
   \___ \ | '_ \ | | | || '_ \ / __|| |/ / |/ _ \ |  < | || || '_ \ | | | | '_ \| |
   ____) || | | || |_| || | | |\__ \|   <| |  __/ | . \| || || | | || |_| | | | | |
  |_____/ |_| |_| \__,_||_| |_||___/|_|\_\_|\___| |_|\_\_||_||_| |_| \__,_|_| |_|_|
                                                                                   
-->

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Shunsuke Kikuchi</title>
    <style>
        /* Reset some default styles */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Body Styling */
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            background-color: #f4f4f4;
            color: #333;
        }

        /* Navigation Styling */
        nav {
            background-color: #333;
            color: #fff;
            padding: 10px 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
        }

        nav .container {
            width: 90%;
            margin: auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        nav ul {
            list-style: none;
            display: flex;
        }

        nav ul li {
            margin-left: 20px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            padding: 5px 10px;
            transition: background 0.3s, color 0.3s;
        }

        nav ul li a:hover {
            background-color: #fff;
            color: #333;
            border-radius: 5px;
        }

        /* Section Styling */
        section {
            padding: 80px 0 40px 0;
            min-height: 100vh;
        }

        .container {
            width: 90%;
            max-width: 1200px;
            margin: auto;
        }

        /* Intro Section */
        #intro {
            
            background: #f4f4f4;
            padding: 40px;
            border-radius: 8px;
            text-align: center;
        }

        #intro h1 {
            margin-bottom: 20px;
            font-size: 2.5em;
            color: #333;
        }

        #intro p {
            margin-bottom: 20px;
            font-size: 1.2em;
        }

        .social-links a {
            margin: 0 10px;
            text-decoration: none;
            color: #333;
            font-weight: bold;
            transition: color 0.3s;
        }

        .social-links a:hover {
            color: #007BFF;
        }

        /* News Section */
        #news ul {
            list-style: none;
            padding: 0;
        }

        #news ul li {
            background: #fff;
            margin-bottom: 10px;
            padding: 15px;
            border-radius: 5px;
            transition: background 0.3s;
        }

        #news ul li:hover {
            background: #e9ecef;
        }

        /* Research Section */
        #research .research-item {
            background: #fff;
            margin-bottom: 20px;
            padding: 20px;
            border-left: 5px solid #007BFF;
            border-radius: 5px;
        }

        /* AI Competitions Section */
        #competitions .competition-item {
            background: #fff;
            margin-bottom: 20px;
            padding: 20px;
            border-left: 5px solid #28a745;
            border-radius: 5px;
        }

        /* Experience Section */
        #experience .experience-item {
            background: #fff;
            margin-bottom: 20px;
            padding: 20px;
            border-left: 5px solid #ffc107;
            border-radius: 5px;
        }

        /* Footer Styling */
        footer {
            background-color: #333;
            color: #fff;
            text-align: center;
            padding: 20px 0;
        }

        footer a {
            color: #fff;
            text-decoration: underline;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            nav .container {
                flex-direction: column;
            }

            nav ul {
                flex-direction: column;
                width: 100%;
            }

            nav ul li {
                margin: 10px 0;
            }

            /* Add this new rule */
            #intro {
                padding-top: 120px;
            }
        }

        /* Smooth Scrolling */
        html {
            scroll-behavior: smooth;
        }

        .competition-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
        }

        .competition-item {
            flex: 1 1 300px;
            background: #fff;
            margin-bottom: 20px;
            padding: 20px;
            border-left: 5px solid #28a745;
            border-radius: 5px;
        }

        .competition-details {
            margin-top: 10px;
        }

        .competition-details ul {
            padding-left: 20px;
        }

        @media screen and (max-width: 767px) {
            .competition-container {
                flex-direction: column;
            }

            .competition-item {
                flex-basis: 100%;
            }

            .competition-details {
                margin-top: 5px;
            }

            h2, h3, h4 {
                font-size: 1.1em;
            }

            p {
                font-size: 0.9em;
            }
        }

        .photos-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin-top: 20px;
        }

        .photo {
            width: auto;
            height: 20%;
            max-width: 200px;
        }
    </style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-E3N4R6FSKV"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-E3N4R6FSKV');
    </script>

</head>
<body>

    <!-- Navigation 
    <nav>
        <div class="container">
            <div class="logo">
                <a href="#intro" style="color: #fff; text-decoration: none; font-size: 1.5em;">Shunsuke Kikuchi</a>
            </div>
            <ul>
                <li><a href="#intro">Intro</a></li>
                <li><a href="#news">News</a></li>
                <li><a href="#research">Research</a></li>
                <li><a href="#competitions">Awards</a></li>
                <li><a href="#experience">Experience</a></li
                <li><a href="#cv">CV</a></li>
            </ul>
        </div>
    </nav>
-->

    <!-- Intro Section -->
    <section id="intro">
        <div class="container">
        <h1></h1>
            <h1>Welcome to My Page !</h1>
            <p>Hi! I'm Shunsuke Kikuchi, a research ML engineer at Jmees Inc, and a project-researcher at National Cancer Center Japan East Hospital. I'm interested in Machine Learning, surgical naviagation, precision medicine and medical robotics. 
                Originally from Tokyo-metoropolitan area, I have been studying in the US since 2021. Aiming to be a researcher in the field of AI in medicine.
                In my free time, I enjoy jogging, manga/anime, and traveling.
                Also as a Kaggle enthusiast, I have participated in various competitions and challenge workshops. Usually choose to work on video-related competition,
                 but I'm also curious about other fields, including 3D reconstruction, segmentation, and reinforcement learning. <br>
                Explore my work, research, and achievements below! </p>
            <div class="social-links">
                <a href="mailto:kikushun2111@gmail.com" target="_blank">Email</a>
                <a href="https://github.com/ShunsukeKikuchi" target="_blank">GitHub</a>
                <a href="https://twitter.com/ne_gi_chi__" target="_blank">Twitter[JP]</a>
                <a href="https://twitter.com/shunkikuch" target="_blank">Twitter[EN]</a>
                <a href="https://www.kaggle.com/shunsukekikuchi" target="_blank">Kaggle</a>
                <a href="https://www.linkedin.com/in/shunsuke-kikuchi-4b9671286/" target="_blank">LinkedIn</a>
            </div>

            <div class="photos-container">
                <img src="headshot.png" alt="Shunsuke Kikuchi Headshot" class="photo">
                <img src="ne_gi_chi__.png" alt="SNN Icon" class="photo">
            </div>
        </div>
    </section>

    <!-- News Section -->
    <section id="news">
        <div class="container">
            <h2>News</h2>
            <ul>
                <li>
                    <strong>December 2025:</strong> Published a technical blog (in Japanese) about our solutions for video understanding competitions. <a href="https://qiita.com/ShunsukeKikuchi/items/53f676a8ae0a3264ee8a" target="_blank">Read the blog</a>
                </li>
                <li>
                    <strong>October 2025:</strong> Hired as a research engineer at Jmees Inc.
                </li>
                <li>
                    <strong>September 2025:</strong> Oral Presentation at 3 MICCAI 2025 Workshop Sessions - SAGES CVS challenge, TopBrain Challenge, and EndoVis25 challenges (OSS, RARE). Presented our challenge winning solutions.
                </li>
                <li>
                    <strong>September 2025:</strong> Poster Session at MICCAI 2025 Workshop Sessions MSB EMERGE. Present our work on Temporal Memory Augmented Module (TMAM) for Consistent Video Segmentation.
                </li>
                <li>
                    <strong>July 2025:</strong> My first paper on Temporal Memory Augmented Module (TMAM) for Consistent Video Segmentation is accepted to MICCAI 2025 workshop MSB EMERGE !
                </li>
                <li>
                    <strong>July 2025:</strong> Start research at National Cancer Center Japan East Hospital Department of Gynecology as a project-researcher.
                </li>
                <li>
                    <strong>June 2025:</strong> Graduate from University of California, Los Angeles with a B.S. in Computational & Systems Biology and B.S. in Applied Mathematics with Honors.
                </li>
                <li>
                    <strong>June 2025:</strong> Poster presentation at The 1st Japan-US Science Forum in Southern California - Memory-Enhanced Temporal Learning: Leveraging SAM2's Memory Modules for Consistent Video Segmentation 
                </li>
                <li>
                    <strong>February 2025:</strong> Poster presentation at The 1st Japan-US Science Forum in Southern California - Memory-Enhanced Temporal Learning: Leveraging SAM2's Memory Modules for Consistent Video Segmentation 
                </li>
                <li>
                    <strong>October 2024:</strong> Oral presentation at MICCAI Workshop Sessions - Endoscopic Vision Challenges. Present our challenge solutions for SegCol, OSS, STIR and PhaKIR challenges.
                </li>
                <li>
                    <strong>August 2024:</strong> Oral presentation at UCLA undergraduate summer research showcase. scGRNdb: A database of gene regulatory networks for single-cell and spatial transcriptomics data.
                </li>
                <li>
                    <strong>August 2024:</strong> Poster presentation at the end of the B.I.G. Summer Research Program. scGRNdb: A database of gene regulatory networks for single-cell and spatial transcriptomics data.
                </li>
                <li>
                    <strong>July 2024:</strong> Finish 27th/1950 in the NeurIPS 2024 - Predict New Medicines with BELKA competition on Kaggle.
                </li>
                <li>
                    <strong>June 2024:</strong> Start internship at Jmees Inc. as a Machine Learning Engineer. Start Summer Resarch Programs (B.I.G. and URC-summer) at UCLA.
                </li>
                <li>
                    <strong>April 2024:</strong> Finish 245th/2767 in the HMS - Harmful Brain Activity Classification competition on Kaggle.
                </li>
                <li>
                    <strong>March 2024:</strong> Finish 4th prize in the 6th National Medical AI Contest in Japan.
            </ul>
        </div>
    </section>

    <!-- Research Section -->
    <section id="research">
        <div class="container">
            <h2>Research</h2>
                <div class="research-item">
                    <h3>Zero-shot Adaption of Image Stiching Towards Endoscopic Surgical Video Understanding</h3>
                    <h4>Jmees Inc.</h4>
                    <p>
                        Effective surgical video understanding requires the clear decomposition of the surgical scene into foreground elements, such as surgical tools, and background anatomical structures. 
                        This research applies image stitching techniques to surgical videos to enable the automated removal and inpainting of specific objects based on ground truth data. 
                        By generating a stabilized, wide-angle view, we aim to enhance the temporal consistency of model predictions for downstream tasks, including segmentation, tracking, and phase classification. 
                        Furthermore, providing contextual information from outside the current frame allows the model to improve its overall predictive accuracy and robustness.
                    </p>
                    <p>Nov 2025 – Present. Leading the project as the first author of the paper.</p>
                </div>
                <div class="research-item">
                    <h3>Phase Recognition of Laparoscopic Hysterectomy Toward Robotic Automation</h3>
                    <h4>National Cancer Center East Hospital, Japan</h4>
                    <p>
                        In total hysterectomy, a uterine manipulator is used to stabilize the uterus and ensure safe resection. 
                        However, during robot-assisted surgery, intraoperative manipulation of the device cannot be controlled directly by the surgeon, highlighting the need for automation. 
                        In this research, we aim to develop a surgical phase classification model that recognizes the manipulator's angle from endoscopic video. 
                        The annotation scheme is being refined through close collaboration and repeated discussion between clinicians and annotators to ensure it is suitable for deep learning models.
                    </p>
                    <p>July 2025 – Present. Working as a project researcher.</p>
                    <!--a href="https://link-to-paper.com" target="_blank" >Read More</a-->
                </div>
                <div class="research-item">
                    <h3>scGRNdb</h3>
                    <h4>Dr. Xia Yang's Lab @ UCLA</h4>
                    <p>SCING is a machine learning model that predicts gene regulatory networks from single-cell & spatial transcriptomics data. 
                        Using GRNs from SCING, we are developing scGRNdb, an analysis pipeline with a database of gene regulatory networks for single-cell and spatial transcriptomics data.
                        I was involved in the part of designing community detection algorithm, pipeline development, and Evaluations.</p>
                    <p> 23 Winter - 25 Spring. Working as an Undergraduate Research Assistant. </p>
                    <!--a href="https://link-to-paper.com" target="_blank" >Read More</a-->
                </div>
                <div class="research-item">
                    <h3>Temporal Memory Augmentation Module (TMAM) </h3>
                    <h4>Jmees Inc.</h4>
                    <p>TMAM a novel framework that adapts any existing 2D segmentation model for video processing by transferring SAM2's memory encoder and attention modules. 
                        TMAM applies a memory encoder to past-frame predictions and uses memory attention to refine current-frame features. By leveraging the inherent temporal redundancy in video sequences, 
                        TMAM captures contextual cues that may be overlooked by single-frame processing, thereby improving robustness to occlusions and boundary artifacts.</p>
                    <p> 24 Winter - 25 Spring. Leading the project as the first author of the paper. </p>
                    <a href="https://openreview.net/pdf?id=UOKCo1qkNU" target="_blank" >Read More</a>
                </div>
                <div class="research-item">
                    <h3>Clinical Trial of Potential Migraine Drug on Rodent Model</h3>
                    <h4>Dr. Jennifer Xie's Lab @ Arkansas State University</h4>
                    <p>This research proposes to investigate the efficacy of RgIA4, a selective α9α10 nicotinic acetylcholine receptor (nAChR) antagonist,
                        in alleviating migraine-related pain using rodent models. The study will evaluate RgIA4's dose- 
                        and time-dependent effects in rat and mouse models by inducing migraines through environmental stress and inflammatory mediators. 
                        The research aims to determine whether blocking α9α10 nAChR can reduce pain and inflammatory responses, potentially offering a new therapeutic approach for migraines with fewer side effects and a reduced risk of medication overuse headaches. </p>
                    <p>21 Fall - 23 Summer. Working as an Undergraduate Research Scholor.</p>
                </div>
            <!-- Add more research items as needed -->
        </div>
    </section>

    <!-- AI Competitions Section -->
    <section id="competitions">
        <div class="container">
            <h2>Kaggle</h2><br>
            <div class="competition-item">
                <h3>NeurIPS 2024 - Predict New Medicines with BELKA</h3>
                <p><b>Silver Medal</b> (27/1950) (<a href="https://www.kaggle.com/competitions/leash-BELKA/leaderboard" target="_blank">Leaderboard</a>)</p>
                <p>Compete with machine learning model which predict small molecule-protein interaction using the Big Encoded Library for Chemical Assessment (BELKA)</p>
                    See more details of our solution <a href="https://www.kaggle.com/competitions/leash-BELKA/discussion/519191" target="_blank">here</a>.
                </div>
            <div class="competition-item">
                    <h3>HMS - Harmful Brain Activity Classification</h3>
                    <p><b>Bronze Medal</b> (245/2767) (<a href="https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/leaderboard" target="_blank">Leaderboard</a>)</p>
                <p>Compete with machine learning mode which classify seizures and other partterns of harmful brain activity in critically ill patients.</p>
                </div>
            <h2>Conference Challenges</h2><br>
            <h3>MICCAI 2025 Challenges</h3><br>
            <div class="competition-container">
                <div class="competition-item">
                    <h3>MICCAI 2025 Lighthouse Challenge — SAGES CVS Challenge <a href="http://cvschallenge.org/miccai-2025" target="_blank">(Official site)</a></h3>
                    <p><strong>Task A (CVS Classification):</strong> <b>2nd Prize</b> ($1,850) &nbsp; | &nbsp; <strong>Task B (Computationally Efficient, CPU-limited):</strong> <b>3rd Prize</b> ($1,000)</p>
                    <div class="competition-details">
                        <p>This Lighthouse challenge was selected for its quality and scale within the MICCAI ecosystem.</p>
                        <ul>
                            <li><strong>Task A:</strong> Trained a 2.5D EVA-02 model in a multi-task setting (segmentation + depth). Careful EMA and hyperparameter tuning improved scores. At inference, compiler-based acceleration was used to meet strict time limits.</li>
                            <li><strong>Task B:</strong> Trained a ResNeXt image classifier with multi-task learning (segmentation + depth). Exported to ONNX, optimized with ONNX-simplifier, and accelerated with ONNX Runtime CPU to satisfy the time constraint.</li>
                        </ul>
                    </div>
                </div>
                <div class="competition-item">
                    <h3>MICCAI 2025 TopBrain Challenge <a href="https://topbrain2025.grand-challenge.org/" target="_blank">(Official site)</a></h3>
                    <p><strong>Segmentation Challenge for Whole Brain Vessel Anatomy</strong> — <b>CTA: 3rd</b>, <b>MTA: 3rd</b></p>
                    <div class="competition-details">
                        <ul>
                            <li>Increased nnUNet training from the default 1000 to 3000 epochs based on prior experience.</li>
                            <li>Despite joining near the deadline and minimal iteration, achieved these results in a one-shot run.</li>
                        </ul>
                    </div>
                </div>
                <div class="competition-item">
                    <h3>MICCAI 2025 EndoVis — Open Suturing Skills Challenge <a href="https://www.synapse.org/Synapse:syn66256386/wiki/" target="_blank">(Official site)</a></h3>
                    <p><strong>GRS:</strong> <b>2nd</b> &nbsp; | &nbsp; <strong>OSATS:</strong> <b>1st</b> &nbsp; | &nbsp; <strong>TRACK:</strong> <b>1st</b></p>
                    <div class="competition-details">
                        <p>Overall objective: automatic assessment of suturing surgical skills.</p>
                        <ul>
                            <li><strong>GRS / OSATS:</strong> Used Swin3D and fine-tuned models trained on keypoints and depth. Added optical flow and foreground-focused views as auxiliary visual features.</li>
                            <li><strong>TRACK:</strong> Used a ConvNeXt encoder with an FPN decoder. Fine-tuned keypoint heads with coordinate MSE and heatmap losses alongside a segmentation pretraining.</li>
                        </ul>
                    </div>
                </div>
                <div class="competition-item">
                    <h3>MICCAI 2025 EndoVis — RARE <a href="https://rare25.grand-challenge.org/" target="_blank">(Official site)</a></h3>
                    <p><strong>3rd</strong></p>
                    <div class="competition-details">
                        <p>Primary challenge: building robust models for extremely rare positive cases with subtle visual cues in real-world settings.</p>
                        <ul>
                            <li>Model 1: Pretrained with Masked Image Modeling on multiple external datasets, then fine-tuned on the challenge dataset.</li>
                            <li>Model 2: Fine-tuned a model pretrained for cancer segmentation.</li>
                            <li>Although the public leaderboard score was modest due to small-sample instability, we trusted the more reliable AUC metric and achieved a significant shake-up on the private leaderboard.</li>
                        </ul>
                    </div>
                </div>
            </div>
            <div class="competition-container">
                <div class="competition-item">
                    <h3>MICCAI Endoscopic Vision Challenge 2024: SegCol</h3>
                    <p><strong>Task 1 & 2:</strong> <b>1st Prize</b> (Task1, 900€) and <b>2nd Prize</b> (Task2, 400€)</p>
                    <div class="competition-details">
                        <p>4-class segmentation challenge for colorectal cancer screening using colonoscopy frames. Developed models to segment anatomy edges and instrument masks.</p>
                        <ul>
                            <li>Task 1: Semantic segmentation for Surgical Tools and Colon Fold Edges</li>
                            <li>Task 2: Selecting set of images from pool to decide which images to annotate (Active Learning)</li>
                        </ul>
                    </div>
                </div>
                <div class="competition-item">
                    <h3>MICCAI Endoscopic Vision Challenge 2024: Open Suturing Skills Challenge</h3>
                    <p><strong>Task 1 & 2:</strong> 3rd Prize (Task1) & 3rd Prize (Task2)</p>
                    <div class="competition-details">
                        <p>Classification of surgical suturing skills using simulated environment videos.</p>
                        <ul>
                            <li>Task 1: Predicting total Global Rating Score (GRS)</li>
                            <li>Task 2: Predicting full OSATS scoring table</li>
                        </ul>
                        <p>For the bost tasks, perform surgical tools segmentation and detect tips of then, then put the movements of them into 1DCNN+GRU model to extract features. Final prediction is performed by GBDT.</p>
                    </div>
                </div>
                <div class="competition-item">
                    <h3>MICCAI Endoscopic Vision Challenge 2024: STIR Challenge</h3>
                    <p><strong>Task 1 & 2:</strong> <b>3rd Prize</b>(2D-tracking, $675) & <b>2nd Prize</b>(3D-tracking, $450)</p>
                    <div class="competition-details">
                        <p>Predicting trajectories in surgical tatoos on tissues in robotic surgery video using the STIR (Surgical Tattoos in Infrared) Dataset.</p>
                        <ul>
                            <li>2D point tracking</li>
                            <li>3D point tracking using stereo-manner left and right videos</li>
                        </ul>
                    </div>
                </div>
            </div>
            <!-- Add more competition items as needed -->
             <div class="competition-item">
                    <h3>6th National Medical AI Contest in Japan</h3>
                    <p><strong>4th prize</strong> (10000 JPY) (<a href="https://www.kaggle.com/competitions/medical-ai-contest2024/leaderboard" target="_blank">Leaderboard</a>)</p>
                    <div class="competition-details">
                        <p>The task was Semantic segmentation of Mutli-Organ, using one of the largest dataset, TotalSegmentator. Segmented organs include
                             gallbladder, liver, pancreas, spleen, kidneys, adrenal glands, aorta, stomach, and duodenum. 
                        </p>
                        <p>For the best model, we employed 3D-UNet approached with large backbone architecutre SeresNext50x4d, with UNet++ and scse block. Achieved >0.89 in mean DSC.
                            Experimental Source Codes and solutions are <a href="https://github.com/ShunsukeKikuchi/MedAI6.git" target="_blank" >here</a></p>
                    </div>
                </div>
        </div>
    </section>

    <!-- Experience Section -->
    <section id="experience">
        <div class="container">
            <h2>Experience</h2><br>
            <div class="experience-item">
                <h3>Internship / ML Engineer</h3>
                <p>Jmees Inc. | June - September 2025</p>
                <ul>
                    <li>Involved in deveoping AI for surgical support systems </li>
                    <li>Partcipated in 3 MICCAI Challenges to give insights for our models</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- CV Section -->
    <section id="cv">
        <div class="container">
            <h2>Curriculum Vitae</h2>
            <p>You can download my CV in PDF format below:</p>
            <a href="CV.pdf" target="_blank" style="display: inline-block; padding: 10px 20px; background-color: #007BFF; color: #fff; text-decoration: none; border-radius: 5px;">Download CV</a>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <p>&copy; 2024 Shunsuke Kikuchi. All rights reserved.</p>
    </footer>
</body>
</html>